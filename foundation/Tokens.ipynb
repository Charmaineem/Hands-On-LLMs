{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzrsQgVD5u8-"
      },
      "source": [
        "# Tokens\n",
        "\n",
        "Models interact with input as small chunks called tokens which can be words, subwords or characters. These are the input and output of the models. In the pipeline, tokenization happens before the input is processed by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV4cElfaJfD-"
      },
      "source": [
        "## Code Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_KhqySsm5mxV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e5ea315f3c564d6f8b23081c5cab7239",
            "ffcb77511b064ae4919e86ee2a790c09",
            "b0c419471eef4f01a11bbeea2e39305a",
            "2d530c2666e54ea59938d987cc3ee1a8",
            "8df66f3ebbdc4969935678d24b3bc324",
            "a5ff432ba64e46ac8419461007437f82",
            "5f194a59fefc4edcae8ab4ba222c75f1",
            "40660d2a06af4aa9a6dfd5c529bc29b9",
            "4e0f682715014ffc8c8a403dada1a41f",
            "38f77ff40da142e597a2b06b8b49a8a1",
            "a97d4146747d44dda2965f067deac212"
          ]
        },
        "id": "D1WPB0mJ7OD8",
        "outputId": "97e0a23c-d1e4-4f3f-9b20-6cbb13d8541b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 36.75it/s]\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'microsoft/Phi-3-mini-4k-instruct',\n",
        "    #device_map='cuda',\n",
        "    torch_dtype='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v1UQehSQ73th"
      },
      "outputs": [],
      "source": [
        "prompt = 'Write a short message telling them that I learn about tokenization today. Explain what it is. <|assistant|>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F77-xL-J8PDG",
        "outputId": "eeafe892-7da8-44f3-a04e-759f020d5ccc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[14350,   263,  3273,  2643, 14509,   963,   393,   306,  5110,  1048,\n",
              "          5993,  2133,  9826, 29889, 12027,  7420,   825,   372,   338, 29889,\n",
              "         29871, 32001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")#.input_ids.to('cuda')\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIjKTo2aK7MG"
      },
      "source": [
        "The output tokens have been converted to their numerical representation and returned as a PyTorch tensor. These are used as the input of the model rather than the actual prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNyeSQwh8e67",
        "outputId": "fc005962-8888-4d96-eec1-9b40c8176779"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[14350,   263,  3273,  2643, 14509,   963,   393,   306,  5110,  1048,\n",
              "          5993,  2133,  9826, 29889, 12027,  7420,   825,   372,   338, 29889,\n",
              "         29871, 32001, 18637,   727, 29991,   306,   925, 10972,  1048,  5993,\n",
              "          2133,  9826, 29889,   739, 29915, 29879,   263, 21028,   262,  1218,\n",
              "          1889,  1304]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generation_output = model.generate(\n",
        "    input_ids=input_ids['input_ids'],\n",
        "    max_new_tokens=20\n",
        ")\n",
        "generation_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Sele5YLVY1"
      },
      "source": [
        "Comparing the input tokens to the generated output, we can see the the model starts generating on token **18637**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqxUKEv98nXn",
        "outputId": "44505f2e-1fa8-4c13-c285-0aaf4075f1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a short message telling them that I learn about tokenization today. Explain what it is. <|assistant|> Hey there! I just learned about tokenization today. It's a fascinating process used\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQL2eHDkAqcO",
        "outputId": "aa112215-2364-4dce-c898-f7b549eefc31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: Write \tToken: 14350\n",
            "Word: a \tToken: 263\n",
            "Word: short \tToken: 3273\n",
            "Word: message \tToken: 2643\n",
            "Word: telling \tToken: 14509\n",
            "Word: them \tToken: 963\n",
            "Word: that \tToken: 393\n",
            "Word: I \tToken: 306\n",
            "Word: learn \tToken: 5110\n",
            "Word: about \tToken: 1048\n",
            "Word: token \tToken: 5993\n",
            "Word: ization \tToken: 2133\n",
            "Word: today \tToken: 9826\n",
            "Word: . \tToken: 29889\n",
            "Word: Exp \tToken: 12027\n",
            "Word: lain \tToken: 7420\n",
            "Word: what \tToken: 825\n",
            "Word: it \tToken: 372\n",
            "Word: is \tToken: 338\n",
            "Word: . \tToken: 29889\n",
            "Word:  \tToken: 29871\n",
            "Word: <|assistant|> \tToken: 32001\n"
          ]
        }
      ],
      "source": [
        "for id in input_ids['input_ids'][0]:\n",
        "  print('Word:', tokenizer.decode(id), '\\tToken:', id.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtquD6bcMM1q"
      },
      "source": [
        "From the output we can analyse each numerical representation that is assigned to each token e.g. **Write - 14350**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIpJYe_7Cb16"
      },
      "source": [
        "## Types of Tokenization\n",
        "There are different ways in which text can be tokenized:\n",
        "1. Word tokens: Splitting text using the whitespace\n",
        "2. Subword tokens: Using full or partial words\n",
        "3. Character tokens: Using unique individual characters in the input\n",
        "4. Byte tokens: Using individual bytes to represent unicode characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Token Embeddings\n",
        "\n",
        "Embeddings - The numerical representation space utilized to capture the meaning and patterns in languages\n",
        "\n",
        "Each model holds the embedding vectors of the tokens in the tokenizer's vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCedr9V2Bw4N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}